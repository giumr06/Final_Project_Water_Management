{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "from preprocessing_uta import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.simplefilter('ignore')\n",
    "RSEED = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/AQUASTAT_complete.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_split_processor = get_pre_split_processor()\n",
    "pre_split_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = pre_split_processor.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df_pre.query(\"year > 2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['gdp_per_capita', 'water_stress', 'total_population_with_access_to_safe_drinking_water']\n",
    "\n",
    "Y = df_pre[targets]\n",
    "X = df_pre.drop(columns=targets)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_pretest, Y_train, Y_pretest = train_test_split(X, Y, test_size=0.30, random_state=RSEED, stratify=X.country)\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(X_pretest, Y_pretest, test_size=0.50, random_state=RSEED, stratify=X_pretest.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_preprocessor = get_minimal_preprocessor(X_train.columns.to_list())\n",
    "\n",
    "X_train_min = minimal_preprocessor.fit_transform(X_train)\n",
    "X_valid_min = minimal_preprocessor.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fully preprocessed variables with country labels\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# X_train_min['country'] = label_encoder.fit_transform(X_train_min['country'])\n",
    "# X_valid_min['country'] = label_encoder.fit_transform(X_valid_min['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_min.shape)\n",
    "print(X_valid_min.shape)\n",
    "# for k,v in X_train_hot.isna().sum().to_dict().items():\n",
    "#     print(k, v)\n",
    "print(\"NaNs in train: \", X_train_min.isna().sum().sum())\n",
    "print(\"NaNs in valid: \", X_valid_min.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Clustering - agglomerative\n",
    "# Import libraries\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'unique_id' by combining 'country' and 'year'\n",
    "X_train_min['unique_id'] = X_train_min['country'] + '_' + X_train_min['year'].astype(str)\n",
    "print(\"NaNs in train: \", X_train_min.isna().sum().sum())\n",
    "\n",
    "# Group the data by \"country\"\n",
    "grouped_data = X_train_min.groupby('country')\n",
    "\n",
    "# Initialize an empty linkage matrix\n",
    "combined_linkage_matrix = None\n",
    "\n",
    "# Initialize an empty list to store country labels\n",
    "country_labels = []\n",
    "\n",
    "# Initialize an empty set to store unique_ids and their corresponding rows\n",
    "unique_id_set = set()\n",
    "\n",
    "# Iterate through each group (country)\n",
    "for country, group in grouped_data:\n",
    "    # Calculate pairwise distances for the group\n",
    "    distances = pdist(group.select_dtypes(include=['number']), metric='euclidean')\n",
    "    \n",
    "    # Create linkage matrix for the group\n",
    "    linkage_matrix = hierarchy.linkage(distances, method='weighted')\n",
    "    \n",
    "    # Add the 'unique_id' labels to the list for each row in the group\n",
    "    unique_ids = group['unique_id'].tolist()\n",
    "\n",
    "\n",
    "   \n",
    "    # for unique_id in unique_ids:\n",
    "    #     if unique_id in unique_id_set:\n",
    "    #         print(f\"Duplicate unique_id: {unique_id}\")\n",
    "    #         print(group[group['unique_id'] == unique_id])\n",
    "    #     else:\n",
    "    #         unique_id_set.add(unique_id)\n",
    "    \n",
    "# Combine the linkage matrices    \n",
    "    if combined_linkage_matrix is None:\n",
    "        combined_linkage_matrix = linkage_matrix\n",
    "    else:\n",
    "        combined_linkage_matrix = hierarchy.linkage(\n",
    "            pd.concat([pd.DataFrame(combined_linkage_matrix), pd.DataFrame(linkage_matrix)]),\n",
    "            method='weighted'\n",
    "        )\n",
    "    \n",
    "    #Add the country labels to the list for each row in the group\n",
    "    country_labels.extend(group['unique_id'].tolist())\n",
    "\n",
    "# # print(country_labels)\n",
    "# #print(combined_linkage_matrix)\n",
    "# # Print counts for various components\n",
    "# print(\"Number of unique IDs:\", len(country_labels))\n",
    "# print(\"Number of rows unique ID:\", X_train_min['unique_id'].count())\n",
    "# print(\"Number of leaves in dendrogram:\", len(combined_linkage_matrix) + 1)\n",
    "\n",
    "# Create a dendrogram for the combined linkage matrix with 'unique_id' labels\n",
    "fig, ax = plt.subplots(figsize=(200, 30))\n",
    "dend = hierarchy.dendrogram(combined_linkage_matrix)#, labels=country_labels)\n",
    "plt.savefig(\"images/mlpr_1804_countries.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old coded with Carmine\n",
    "\n",
    "\n",
    " # create id for country and year (country label and year) \n",
    "# X_train_min['unique_id'] = X_train_min['country'] + '_' + X_train_min['year'].astype(str)\n",
    "\n",
    "# # Group the data by \"country\"\n",
    "# grouped_data = X_train_min.groupby('country')\n",
    "\n",
    "# # Initialize an empty linkage matrix\n",
    "# combined_linkage_matrix = None\n",
    "\n",
    "# # Initialize an empty list to store country labels\n",
    "# country_labels = grouped_data.first().index.tolist()\n",
    "# #country_labels\n",
    "\n",
    "# all_group_distances = []\n",
    "# # Iterate through each group (country)\n",
    "# for country, group in grouped_data:\n",
    "#     #print(country, group)\n",
    "#     #print(group.shape)\n",
    "#     #print(group.iloc[:,2:-1].to_numpy())\n",
    "#     # Calculate pairwise distances for the group\n",
    "#     #print(type(group.iloc[:,2:].to_numpy()))\n",
    "\n",
    "#     distances = pdist(np.asarray(group.iloc[:,2:-1]))\n",
    "#     all_group_distances.append(distances)\n",
    "\n",
    "# print(len(all_group_distances))\n",
    "# #print(array_distances.shape)\n",
    "# #     # Create linkage matrix for the group\n",
    "# #linkage_matrix = hierarchy.linkage(array_distances, method='weighted')\n",
    "    \n",
    "# #     # Combine the linkage matrices\n",
    "# #     if combined_linkage_matrix is None:\n",
    "# #         combined_linkage_matrix = linkage_matrix\n",
    "# #     else:\n",
    "# #         combined_linkage_matrix = hierarchy.linkage(\n",
    "# #             pd.concat([pd.DataFrame(combined_linkage_matrix), pd.DataFrame(linkage_matrix)]),\n",
    "# #             method='weighted'\n",
    "# #         )\n",
    "\n",
    "# # # Print the number of labels and the number of leaves in the dendrogram\n",
    "# # print(\"Number of labels:\", len(country_labels))\n",
    "# # print(\"Number of leaves in dendrogram:\", len(combined_linkage_matrix) + 1)\n",
    "\n",
    "# # # Create a dendrogram for the combined linkage matrix with country labels\n",
    "# # fig, ax = plt.subplots(figsize=(200, 30))\n",
    "# # dend = hierarchy.dendrogram(combined_linkage_matrix, labels=X_train_min['unique_id'])\n",
    "# # plt.savefig(\"images/mlpr_1804_countries.png\", dpi=300)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow Method to determine the optimal number of clusters\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.patch.set_facecolor('#f6f5f5')\n",
    "plt.plot(range(1, 25), clusters)\n",
    "plt.title('The Elbow Method', fontsize=20)\n",
    "plt.xlabel('No. of Clusters')\n",
    "plt.ylabel('Inertia (Within-cluster Sum of Squares)')\n",
    "fig.text(0.5, 0.4, \"Identify the best K-value for dummified countries\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette scores (to be adapted later )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeled Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Silhouette Score for Optimal K with Numeric Labels (Scaled Data)\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "K_max = 15  # Maximum number of clusters to consider\n",
    "for k in range(2, K_max + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RSEED)\n",
    "    cluster_labels = kmeans.fit_predict(X_train_lab)\n",
    "    silhouette_avg = silhouette_score(X_train_lab, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Store the cluster labels for further use\n",
    "cluster_labels_numeric = cluster_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Silhouette Scores to find the optimal K\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(2, K_max + 1), silhouette_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal K with scaled variables incl. one numeric country variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Silhouette Scores for different K values\n",
    "print(\"Silhouette Scores:\", silhouette_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
